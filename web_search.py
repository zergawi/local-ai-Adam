# tools/web_search.py

import requests
from bs4 import BeautifulSoup
from langchain.tools import tool
import urllib.parse
import json
import time


class WebSearcher:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def google_search(self, query, num_results=5, lang='ar'):
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        try:
            # ØªØ´ÙÙŠØ± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            encoded_query = urllib.parse.quote_plus(query)
            url = f"https://www.google.com/search?q={encoded_query}&hl={lang}&num={num_results}"

            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            results = []

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«
            search_results = soup.find_all('div', class_='g')

            for result in search_results[:num_results]:
                title_elem = result.find('h3')
                link_elem = result.find('a')
                snippet_elem = result.find('span', class_=['aCOpRe', 'st'])

                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    if link.startswith('/url?q='):
                        link = urllib.parse.unquote(link.split('/url?q=')[1].split('&')[0])

                    if link.startswith('http'):
                        results.append({
                            'title': title,
                            'link': link,
                            'snippet': snippet
                        })

            return results

        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google: {e}")
            return []

    def get_website_content(self, url, max_chars=2000):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆÙ‰ Ù…ÙˆÙ‚Ø¹ ÙˆÙŠØ¨"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨ ÙÙŠÙ‡Ø§
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            main_content = soup.find('main') or soup.find('article') or soup.find('div',
                                                                                  class_=['content', 'main-content',
                                                                                          'post-content'])

            if main_content:
                text = main_content.get_text(strip=True, separator=' ')
            else:
                text = soup.get_text(strip=True, separator=' ')

            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·ÙˆÙ„
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            clean_text = ' '.join(lines)

            if len(clean_text) > max_chars:
                clean_text = clean_text[:max_chars] + "..."

            return clean_text

        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ {url}: {e}")
            return f"Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {str(e)}"


# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù…Ù† ÙØ¦Ø© Ø§Ù„Ø¨Ø­Ø«
web_searcher = WebSearcher()


@tool
def search_google(query: str, num_results: int = 3) -> str:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª. Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª."""
    try:
        results = web_searcher.google_search(query, num_results)

        if not results:
            return f"Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù†: {query}"

        response = f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† '{query}':\n\n"

        for i, result in enumerate(results, 1):
            response += f"{i}. {result['title']}\n"
            if result['snippet']:
                response += f"   {result['snippet']}\n"
            response += f"   Ø§Ù„Ø±Ø§Ø¨Ø·: {result['link']}\n\n"

        return response

    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {str(e)}"


@tool
def get_website_info(url: str) -> str:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ù…ÙˆÙ‚Ø¹ ÙˆÙŠØ¨ Ù…Ø¹ÙŠÙ†. Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ ØµÙØ­Ø© ÙˆÙŠØ¨ Ù…Ø­Ø¯Ø¯Ø©."""
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        content = web_searcher.get_website_content(url)
        return f"Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ {url}:\n\n{content}"

    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {str(e)}"


@tool
def search_and_read(query: str, read_first: bool = True) -> str:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google ÙˆÙ‚Ø±Ø§Ø¡Ø© Ø£ÙˆÙ„ Ù†ØªÙŠØ¬Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹. Ù…ÙÙŠØ¯ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø© Ø­ÙˆÙ„ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹ÙŠÙ†."""
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø£ÙˆÙ„Ø§Ù‹
        results = web_searcher.google_search(query, 3)

        if not results:
            return f"Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù†: {query}"

        response = f"Ø¨Ø­Ø«Øª Ø¹Ù† '{query}' ÙˆÙˆØ¬Ø¯Øª:\n\n"

        if read_first and results:
            # Ù‚Ø±Ø§Ø¡Ø© Ø£ÙˆÙ„ Ù†ØªÙŠØ¬Ø©
            first_result = results[0]
            response += f"Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {first_result['title']}\n"
            response += f"Ø§Ù„Ø±Ø§Ø¨Ø·: {first_result['link']}\n\n"

            content = web_searcher.get_website_content(first_result['link'])
            response += f"Ø§Ù„Ù…Ø­ØªÙˆÙ‰:\n{content}\n\n"

            # Ø¹Ø±Ø¶ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            if len(results) > 1:
                response += "Ù†ØªØ§Ø¦Ø¬ Ø£Ø®Ø±Ù‰:\n"
                for i, result in enumerate(results[1:], 2):
                    response += f"{i}. {result['title']}\n"
                    response += f"   {result['link']}\n"
        else:
            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙ‚Ø·
            for i, result in enumerate(results, 1):
                response += f"{i}. {result['title']}\n"
                if result['snippet']:
                    response += f"   {result['snippet']}\n"
                response += f"   {result['link']}\n\n"

        return response

    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ù‚Ø±Ø§Ø¡Ø©: {str(e)}"


@tool
def get_news(topic: str = "Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ…", language: str = "ar") -> str:
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­ÙˆÙ„ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹ÙŠÙ†."""
    try:
        if language == "ar":
            search_query = f"{topic} Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ…"
        else:
            search_query = f"{topic} news today"

        results = web_searcher.google_search(search_query, 5)

        if not results:
            return f"Ù„Ù… Ø£Ø¬Ø¯ Ø£Ø®Ø¨Ø§Ø±Ø§Ù‹ Ø­ÙˆÙ„: {topic}"

        response = f"Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­ÙˆÙ„ '{topic}':\n\n"

        for i, result in enumerate(results, 1):
            response += f"ğŸ“° {result['title']}\n"
            if result['snippet']:
                response += f"   {result['snippet']}\n"
            response += f"   Ø§Ù„Ù…ØµØ¯Ø±: {result['link']}\n\n"

        return response

    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±: {str(e)}"